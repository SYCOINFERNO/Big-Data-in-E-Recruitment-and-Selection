## Big-Data-in-E-Recruitment-and-Selection

The exponential growth of digital data necessitates efficient information retrieval and processing systems in the recruitment and Human Resources (HR) sectors. This project proposes a highly scalable, deterministic, and rule-based architecture for resume parsing, storage, and candidate selection that explicitly excludes Artificial Intelligence and Machine Learning. Utilizing the Apache Hadoop ecosystem, unstructured resume data is ingested and stored in the Hadoop Distributed File System (HDFS). Apache Tika is employed to extract text from diverse document formats, while MapReduce handles distributed data processing. To query the stored data, Apache Hive is used to provide structured, SQL-like querying capabilities. For candidate retrieval, the system integrates Apache Solr to perform inverted-index full-text searches using strict Boolean logic and exact keyword matching, ensuring high-precision filtering. Finally, HR data linking and candidate ranking are achieved through deterministic record matching and a transparent Weighted Scoring Model. This approach guarantees a scalable, fault-tolerant, and objective candidate screening process without relying on opaque AI algorithms or probabilistic predictive models.
